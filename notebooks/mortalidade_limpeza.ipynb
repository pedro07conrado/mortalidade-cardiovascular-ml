{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bc64213",
   "metadata": {},
   "source": [
    "## Limpeza, padronização e transformação dos CSV \"Mortalidade_Geral\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44115ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import duckdb\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "# Suprimir warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configurar caminhos\n",
    "RAW_PATH = Path('../data/raw')\n",
    "PROCESSED_PATH = Path('../data/processed')\n",
    "PROCESSED_PATH.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "\n",
    "COLUNAS_INTERESSE = ['CODMUNRES', 'CAUSABAS', 'DTOBITO']\n",
    "\n",
    "print(\"Ambiente configurado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff728507",
   "metadata": {},
   "source": [
    "Como o arquivo é muito grande, essa função lê o arquivo de mortalidade em pedaços de 100.000 linhas, filtra o que é cardio e dps agrega, para não correr o risco de maquina fracas como a minha não consiga terminar o processo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246a45c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processar_mortalidade(ano):\n",
    "    print(f'\\n=== Processando Mortalidade {ano} ===')\n",
    "    \n",
    "    # Tenta padroes de nome comuns do Datasus\n",
    "    file_path = RAW_PATH / f'mortalidade_geral{ano}.csv'\n",
    "    \n",
    "    if not file_path.exists():\n",
    "        print(f'[ERRO] Arquivo nao encontrado: {file_path}')\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    print(f'Lendo arquivo: {file_path}')\n",
    "\n",
    "    chunks = []\n",
    "    \n",
    "    # Ler o arquivo em pedacos (chunks) de 100 mil linhas\n",
    "    try:\n",
    "        iterator = pd.read_csv(\n",
    "            file_path, \n",
    "            sep=';', \n",
    "            usecols=COLUNAS_INTERESSE,\n",
    "            encoding='latin1',\n",
    "            dtype={'CODMUNRES': str, 'CAUSABAS': str},\n",
    "            chunksize=100000,\n",
    "            low_memory=False\n",
    "        )\n",
    "        \n",
    "        for i, chunk in enumerate(iterator):\n",
    "            # 1. Filtrar apenas Doencas Cardiovasculares\n",
    "            # Capitulo IX do CID-10 vai de I00 a I99. \n",
    "            # Entao pegamos tudo que comeca com 'I'\n",
    "            chunk = chunk.dropna(subset=['CAUSABAS'])\n",
    "            chunk_cardio = chunk[chunk['CAUSABAS'].str.startswith('I', na=False)].copy()\n",
    "            \n",
    "            if not chunk_cardio.empty:\n",
    "                # 2. Padronizar Municipio (6 digitos)\n",
    "                chunk_cardio['codmun'] = chunk_cardio['CODMUNRES'].str.slice(0, 6)\n",
    "                \n",
    "                # 3. Contar obitos por municipio neste pedaco\n",
    "                # Agrupamos agora para reduzir o tamanho dos dados drasticamente\n",
    "                resumo_chunk = chunk_cardio.groupby('codmun').size().reset_index(name='obitos')\n",
    "                chunks.append(resumo_chunk)\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f'[ERRO] Falha ao ler arquivo: {e}')\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Consolidar todos os pedacos do ano\n",
    "    if chunks:\n",
    "        df_ano = pd.concat(chunks, ignore_index=True)\n",
    "        \n",
    "        # Re-agrupar para somar os totais dos chunks\n",
    "        df_ano = df_ano.groupby('codmun')['obitos'].sum().reset_index()\n",
    "        df_ano.rename(columns={'obitos': 'mortes_cardio'}, inplace=True)\n",
    "        \n",
    "        df_ano['ano'] = int(ano)\n",
    "        \n",
    "        print(f'Total de obitos cardio processados em {ano}: {df_ano[\"mortes_cardio\"].sum()}')\n",
    "        return df_ano\n",
    "    else:\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c398c888",
   "metadata": {},
   "source": [
    "Consolidação "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4aae2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_mortalidade = []\n",
    "\n",
    "# Processar os anos definidos\n",
    "for ano in [2007, 2010, 2015]:\n",
    "    df_temp = processar_mortalidade(ano)\n",
    "    if not df_temp.empty:\n",
    "        dfs_mortalidade.append(df_temp)\n",
    "\n",
    "# Juntar tudo\n",
    "if dfs_mortalidade:\n",
    "    df_final_mort = pd.concat(dfs_mortalidade, ignore_index=True)\n",
    "    \n",
    "    # Filtrar codigos de municipio invalidos (tamanho diferente de 6)\n",
    "    df_final_mort = df_final_mort[df_final_mort['codmun'].str.len() == 6]\n",
    "    \n",
    "    print('\\n=== Dataset Mortalidade Consolidado ===')\n",
    "    print(f'Shape: {df_final_mort.shape}')\n",
    "    print(df_final_mort.head())\n",
    "    \n",
    "    # Salvar Parquet\n",
    "    output_file = PROCESSED_PATH / 'mortalidade_cardio_clean.parquet'\n",
    "    df_final_mort.to_parquet(output_file, index=False)\n",
    "    print(f'[SUCESSO] Salvo em: {output_file}')\n",
    "else:\n",
    "    print('[ERRO] Nenhum dado processado.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8aafe6",
   "metadata": {},
   "source": [
    "SQL só para visualizar se está tudo nos conformes kkkk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff255a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n=== Validacao com DuckDB ===')\n",
    "\n",
    "con = duckdb.connect()\n",
    "con.register('tb_mortalidade', df_final_mort)\n",
    "\n",
    "# 1. Resumo por Ano (Verificar se tem dados para todos os anos)\n",
    "query_resumo = \"\"\"\n",
    "SELECT \n",
    "    ano, \n",
    "    COUNT(*) as qtd_municipios_com_obito,\n",
    "    SUM(mortes_cardio) as total_obitos_brasil,\n",
    "    AVG(mortes_cardio)::INT as media_obitos_cidade,\n",
    "    MAX(mortes_cardio) as max_obitos_cidade\n",
    "FROM tb_mortalidade\n",
    "GROUP BY ano\n",
    "ORDER BY ano\n",
    "\"\"\"\n",
    "print('\\n[SQL] Resumo de Mortalidade Cardiovascular:')\n",
    "print(con.execute(query_resumo).df())\n",
    "\n",
    "# 2. Checagem Top 5 Cidades (Validacao de consistencia - SP e Rio devem liderar)\n",
    "query_top = \"\"\"\n",
    "SELECT codmun, mortes_cardio\n",
    "FROM tb_mortalidade\n",
    "WHERE ano = 2015\n",
    "ORDER BY mortes_cardio DESC\n",
    "LIMIT 5\n",
    "\"\"\"\n",
    "print('\\n[SQL] Top 5 Cidades com mais obitos cardio (2015):')\n",
    "print(con.execute(query_top).df())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
